{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from typing import List\n",
    "\n",
    "from copy import copy\n",
    "from itertools import combinations, permutations\n",
    "import pickle\n",
    "from random import choice, random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an entry exercise, we will try to train the simple neural network able to sort the 1D array step by step. The inference of the model would be the most optimal permutation within given possible actions (we will use neighbors transpositions).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the target for the greedy algorithm\n",
    "The 'sort loss' function we use \n",
    "\n",
    "$$\n",
    "\\text{SL} = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} [x_i > x_j]\n",
    "$$\n",
    "\n",
    "This way we get a convex function to make sure it's optimizable.\n",
    "Below is the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_loss(input: List[int]) -> float:\n",
    "    loss = 0\n",
    "    for i in range(len(input)):\n",
    "        for j in range(i, len(input)):\n",
    "            if input[i] > input[j]:\n",
    "                loss += 1\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_loss([1,5,4,3,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The available actions are narrowed down to transpositions only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actions_possible(input: List[int]) -> List[List[int]]:\n",
    "    res = []\n",
    "    for i in range(len(input)-1):\n",
    "        input_copy = copy(input)\n",
    "        input_copy[i], input_copy[i+1] = input_copy[i+1], input_copy[i]\n",
    "        res.append(input_copy)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train data can be easily generated for the greedy algorithm and fixed sequence length as the most optimal action in terms of previously defined 'sort loss':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate simple data\n",
    "SOURCE = [x for x in range(7)]\n",
    "DATASET_SIZE = 500\n",
    "SEQ_LEN = 4\n",
    "\n",
    "DATA = []\n",
    "\n",
    "SAMPLES = list(permutations(SOURCE, SEQ_LEN))\n",
    "_picked = set()\n",
    "for _ in range(DATASET_SIZE):\n",
    "    v = choice(SAMPLES)\n",
    "    if v not in _picked:\n",
    "        _picked.add(v)\n",
    "\n",
    "    swap_chosen = None\n",
    "    for swap in get_actions_possible(list(v)):\n",
    "        if swap_chosen is None:\n",
    "            swap_chosen = swap\n",
    "        if sort_loss(swap) < sort_loss(swap_chosen):\n",
    "            swap_chosen = swap\n",
    "    \n",
    "    DATA.append((list(v), swap_chosen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "\n",
      "([5, 6, 2, 4], [5, 2, 6, 4])\n",
      "------------------\n",
      "\n",
      "[6, 5, 2, 4] 5\n",
      "[5, 2, 6, 4] 3\n",
      "[5, 6, 4, 2] 5\n",
      "-------------------\n",
      "\n",
      "------------------\n",
      "\n",
      "([0, 3, 5, 6], [3, 0, 5, 6])\n",
      "------------------\n",
      "\n",
      "[3, 0, 5, 6] 1\n",
      "[0, 5, 3, 6] 1\n",
      "[0, 3, 6, 5] 1\n",
      "-------------------\n",
      "\n",
      "------------------\n",
      "\n",
      "([3, 0, 2, 6], [0, 3, 2, 6])\n",
      "------------------\n",
      "\n",
      "[0, 3, 2, 6] 1\n",
      "[3, 2, 0, 6] 3\n",
      "[3, 0, 6, 2] 3\n",
      "-------------------\n",
      "\n",
      "------------------\n",
      "\n",
      "([3, 4, 6, 0], [3, 4, 0, 6])\n",
      "------------------\n",
      "\n",
      "[4, 3, 6, 0] 4\n",
      "[3, 6, 4, 0] 4\n",
      "[3, 4, 0, 6] 2\n",
      "-------------------\n",
      "\n",
      "------------------\n",
      "\n",
      "([6, 3, 4, 5], [3, 6, 4, 5])\n",
      "------------------\n",
      "\n",
      "[3, 6, 4, 5] 2\n",
      "[6, 4, 3, 5] 4\n",
      "[6, 3, 5, 4] 4\n",
      "-------------------\n",
      "\n",
      "------------------\n",
      "\n",
      "([5, 3, 1, 4], [3, 5, 1, 4])\n",
      "------------------\n",
      "\n",
      "[3, 5, 1, 4] 3\n",
      "[5, 1, 3, 4] 3\n",
      "[5, 3, 4, 1] 5\n",
      "-------------------\n",
      "\n",
      "------------------\n",
      "\n",
      "([2, 3, 5, 4], [2, 3, 4, 5])\n",
      "------------------\n",
      "\n",
      "[3, 2, 5, 4] 2\n",
      "[2, 5, 3, 4] 2\n",
      "[2, 3, 4, 5] 0\n",
      "-------------------\n",
      "\n",
      "------------------\n",
      "\n",
      "([2, 1, 6, 3], [1, 2, 6, 3])\n",
      "------------------\n",
      "\n",
      "[1, 2, 6, 3] 1\n",
      "[2, 6, 1, 3] 3\n",
      "[2, 1, 3, 6] 1\n",
      "-------------------\n",
      "\n",
      "------------------\n",
      "\n",
      "([1, 2, 6, 5], [1, 2, 5, 6])\n",
      "------------------\n",
      "\n",
      "[2, 1, 6, 5] 2\n",
      "[1, 6, 2, 5] 2\n",
      "[1, 2, 5, 6] 0\n",
      "-------------------\n",
      "\n",
      "------------------\n",
      "\n",
      "([5, 3, 6, 1], [3, 5, 6, 1])\n",
      "------------------\n",
      "\n",
      "[3, 5, 6, 1] 3\n",
      "[5, 6, 3, 1] 5\n",
      "[5, 3, 1, 6] 3\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, row in enumerate(DATA):\n",
    "    if i > 9:\n",
    "        break\n",
    "    print('------------------\\n')\n",
    "    print(row)\n",
    "    print('------------------\\n')\n",
    "    for v in get_actions_possible(row[0]):\n",
    "        print(v, sort_loss(v))\n",
    "    print('-------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_RATE = 0.9\n",
    "train_data = []\n",
    "test_data = []\n",
    "for row in DATA:\n",
    "    if random() < SPLIT_RATE:\n",
    "        train_data.append(row)\n",
    "    else:\n",
    "        test_data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/train.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "\n",
    "with open('../data/test.pkl', 'wb') as f:\n",
    "    pickle.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/train.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "with open('../data/test.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a simple sorter neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SorterNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SorterNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train_data = torch.tensor([x[0] for x in train_data], dtype=torch.float32)\n",
    "output_train_data = torch.tensor([x[1] for x in train_data], dtype=torch.float32)\n",
    "\n",
    "input_test_data = torch.tensor([x[0] for x in test_data], dtype=torch.float32)\n",
    "output_test_data = torch.tensor([x[1] for x in test_data], dtype=torch.float32)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = input_train_data.shape[1]\n",
    "output_size = output_train_data.shape[1]\n",
    "hidden_size = 64\n",
    "model = SorterNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/5000], Loss: 0.1867\n",
      "Epoch [2000/5000], Loss: 0.0803\n",
      "Epoch [3000/5000], Loss: 0.0554\n",
      "Epoch [4000/5000], Loss: 0.0430\n",
      "Epoch [5000/5000], Loss: 0.0331\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(input_train_data)\n",
    "    loss = criterion(outputs, output_train_data)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5, 3, 1, 4], [3, 5, 1, 4])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: [[3. 5. 1. 4.]]\n"
     ]
    }
   ],
   "source": [
    "test_input = torch.tensor([[5, 3, 1, 4]], dtype=torch.float32)\n",
    "predicted_output = model(test_input).round()\n",
    "print(\"Predicted Output:\", predicted_output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 3, 1, 4] 4\n",
      "[3, 1, 5, 4] 2\n",
      "[3, 5, 4, 1] 4\n"
     ]
    }
   ],
   "source": [
    "for v in get_actions_possible([3, 5, 1, 4]):\n",
    "    print(v, sort_loss(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: [[3. 1. 5. 4.]]\n"
     ]
    }
   ],
   "source": [
    "test_input = torch.tensor([[3, 5, 1, 4]], dtype=torch.float32)\n",
    "predicted_output = model(test_input).round()\n",
    "print(\"Predicted Output:\", np.round(predicted_output.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\t [5, 3, 1, 4]\n",
      "Step 1:\t [3, 5, 1, 4] \tSort loss: 3\n",
      "Step 2:\t [3, 1, 5, 4] \tSort loss: 2\n",
      "Step 3:\t [1, 3, 5, 4] \tSort loss: 1\n",
      "Step 4:\t [1, 3, 4, 5] \tSort loss: 0\n"
     ]
    }
   ],
   "source": [
    "sample = test_data[0][0]\n",
    "v = sort_loss(sample)\n",
    "\n",
    "print(f'Input:\\t {sample}')\n",
    "i = 1\n",
    "while v > 0:\n",
    "    test_input = torch.tensor([sample], dtype=torch.float32)\n",
    "    predicted_output = model(test_input).round()\n",
    "    sample = predicted_output.detach().numpy().astype(int).tolist()[0]\n",
    "    v = sort_loss(sample)\n",
    "    print(f'Step {i}:\\t', sample, f'\\tSort loss: {v}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like now we have a model that has trained the best movements for given permutations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to consider:\n",
    "1. **what if the input consists of random unexpected values?**\n",
    "   <br>_If the elements live in a metric space we assume them as sortable and thus, they can be indexed with known/expected intervals and sorted after that._ \n",
    "2. **what if the input is very large?**\n",
    "   <br>_This can be handled by eating the elephant one bite at a time. We can break the input into acceptable parts and sort them independently. Once all parts are sorted we apply the merge-sort strategy. If the chunks are stil very big we can apply the same algorithm recursively. Yet the solution is designed for the fixed input only which is already a limitation._\n",
    "3. **how to define sort loss and possible actions for a more sophisticated group?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
